{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 데이터 다운로드 하기\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import urllib.request\n",
    "\n",
    "#  URL 과 저장 경로 지정\n",
    "url = \"http://uta.pw/shodou/img/28/214.png\"\n",
    "savename = \"./test.png\"\n",
    "\n",
    "# 다운로드 하기\n",
    "urllib.request.urlretrieve(url,savename)\n",
    "print(\"저장 되었습니다!\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "저장 되었습니다!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import urllib.request\n",
    "\n",
    "#  URL 과 저장 경로 지정\n",
    "url = \"http://uta.pw/shodou/img/28/214.png\"\n",
    "savename = \"./test.png\"\n",
    "\n",
    "# 다운로드 하기\n",
    "\n",
    "mem = urllib.request.urlopen(url).read()\n",
    "\n",
    "#  파일로 저장하기 \n",
    "with open(savename, mode=\"wb\") as f:\n",
    "    f.write(mem)\n",
    "    print(\"저장 완료! \")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# BeautifulSoup -> Scrapping 하기\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "#  기본 사용법\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# HTML Sample\n",
    "\n",
    "html = \"\"\"\n",
    "    <html>\n",
    "        <body>\n",
    "            <h1>Header</h1>\n",
    "            <p> Line 1을 서술 </p>\n",
    "        </body>\n",
    "    </html>\n",
    "\"\"\"\n",
    "\n",
    "# HTML 분석\n",
    "soup = BeautifulSoup(html , 'html.parser')\n",
    "# print(soup)\n",
    "\n",
    "# 원하는 부분 추출하기\n",
    "h1 = soup.html.body.h1\n",
    "p1 = soup.html.body.p\n",
    "print(h1)\n",
    "\n",
    "# Text 만 출력\n",
    "print(\"h1=\" , h1.string)\n",
    "print(\"h1=\" , h1.text)\n",
    "print(\"p1=\" , p1.string)\n",
    "print(\"p1=\" , p1.text)\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<h1>Header</h1>\n",
      "h1= Header\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# id로 요소를 추출하기\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# HTML sample\n",
    "\n",
    "html = \"\"\"\n",
    "    <html>\n",
    "        <body>\n",
    "            <h1 id=\"title\">Header </h1>\n",
    "            <p id=\"body\">Line 1 description </p>\n",
    "            <span class=\"whatever\">what the fucl?</span>\n",
    "        </body>\n",
    "    </html>\n",
    "\"\"\"\n",
    "\n",
    "# HTML analystic\n",
    "soup = BeautifulSoup(html , \"html.parser\")\n",
    "\n",
    "# extract where i want\n",
    "\n",
    "title = soup.find(id= \"title\")\n",
    "body = soup.find(id=\"body\")\n",
    "\n",
    "print(title)\n",
    "print(body)\n",
    "\n",
    "# text 만 출력\n",
    "print(\"span = \", soup.find(\"span\" , {\"class\" : \"whatever\"}).text)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<h1 id=\"title\">Header </h1>\n",
      "<p id=\"body\">Line 1 description </p>\n",
      "span =  what the fucl?\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# 여러개의 요소 추출하기\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html =  \"\"\"\n",
    "    <html>\n",
    "        <body>\n",
    "            <ul>\n",
    "                <li><a href=\"http://www.naver.com\">naver</a></li>\n",
    "                <li><a href=\"http://www.daum.net\">daum</a></li>\n",
    "            </ul>\n",
    "        </body>\n",
    "    </html>\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# HTML analystic\n",
    "\n",
    "soup = BeautifulSoup(html , 'html.parser')\n",
    "links = soup.find_all('a')\n",
    "\n",
    "# 링크 목록 출력 하기\n",
    "for link in links:\n",
    "    #  해당하는 속성 들고 오기\n",
    "    hrefTag = link.attrs['href']\n",
    "    print(hrefTag)\n",
    "    \n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "http://www.naver.com\n",
      "http://www.daum.net\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "http  -> http://www.weather.go.kr/weather/forecast/mid-term-rss3.jsp?stnId=109"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 기상청 자료 활용하기\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "url = \"http://www.weather.go.kr/weather/forecast/mid-term-rss3.jsp?stnId=109\"\n",
    "\n",
    "# DATA 가져오기\n",
    "\n",
    "res = req.urlopen(url)\n",
    "soup = BeautifulSoup(res , \"html.parser\")\n",
    "\n",
    "# 원하는 데이터 추출 하기\n",
    "title = soup.find(\"title\").string\n",
    "info = soup.find(\"wf\").string\n",
    "\n",
    "infoSorts = info.split(\"<br />\")\n",
    "\n",
    "for infoSort in infoSorts:\n",
    "    print(infoSort)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CSS 선택자 사용하기\n",
    "\n",
    "soup.select_one() : css 선택자로 요소 하나를 추출\n",
    "soup.select() : css 선택자로 요소 여러개를 리스트로 추출\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = \"\"\"\n",
    "    <html>\n",
    "        <body>\n",
    "            <div id= \"meigen\">\n",
    "                <h1>위키 북스</h1>\n",
    "                <ul class=\"items\">\n",
    "                    <li>유니티 게임 이펙트 입문서</li>\n",
    "                    <li>스위프트로 시작하는 아이폰 앱 개발 교과서</li>\n",
    "                    <li>모던 웹사이트 디자인의 정석</li>\n",
    "                </ul>\n",
    "            </div>\n",
    "        </body>\n",
    "    </html>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html , \"html.parser\")\n",
    "\n",
    "# 필요한 부분 css 로 추출하기\n",
    "# title 부분 추출하기\n",
    "\n",
    "\n",
    "# \"id : #\", \"class :.\"> : 자손,\" \": 후손\"\n",
    "h1 = soup.select_one(\"div#meigen > h1\")\n",
    "print(h1)\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<h1>위키 북스</h1>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "# 목록 부분 추출하기\n",
    "li_lists = soup.select(\"div#meigen > ul.items > li\")\n",
    "# print(li_lists)\n",
    "\n",
    "for li_list in li_lists:\n",
    "    print(li_list.string)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "유니티 게임 이펙트 입문서\n",
      "스위프트로 시작하는 아이폰 앱 개발 교과서\n",
      "모던 웹사이트 디자인의 정석\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "source": [
    "# 네이버 금융에서 환율 정보 추출 하기\n",
    "\n",
    "import urllib.request as req\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "url = \"https://finance.naver.com/marketindex/\"\n",
    "\n",
    "\n",
    "\n",
    "res = req.urlopen(url)\n",
    "\n",
    "soup = BeautifulSoup(res , \"html.parser\" )\n",
    "\n",
    "titles = soup.select(\"h3.h_lst > span.blind\")\n",
    "\n",
    "for title in titles[0:4]:\n",
    "    print(title.string)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "¹Ì±¹ USD\n",
      "ÀÏº» JPY(100¿£)\n",
      "À¯·´¿¬ÇÕ EUR\n",
      "Áß±¹ CNY\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "prices = soup.select(\"div.head_info > span.value\")\n",
    "price_array = []\n",
    "country_array = [\"미국 : \" , \"일본 : \" , \"유럽연합 : \" , \"중국 : \"]\n",
    "for price in prices[:4]:\n",
    "    price_array.append(price.string)\n",
    "    \n",
    "for i in range(len(price_array)):\n",
    "    print(country_array[i] , price_array[i])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "미국 :  1,143.30\n",
      "일본 :  1,035.64\n",
      "유럽연합 :  1,357.61\n",
      "중국 :  176.72\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "source": [
    "\n",
    "import urllib.request as req\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://movie.daum.net/ranking/boxoffice/yearly\"\n",
    "\n",
    "res = req.urlopen(url)\n",
    "\n",
    "soup = BeautifulSoup(res , \"html.parser\")\n",
    "\n",
    "titles = soup.select(\"a.link_txt\")\n",
    "\n",
    "title_array = []\n",
    "\n",
    "for title in titles[:50]:\n",
    "    title_array.append(title.string)\n",
    "\n",
    "for i in range(len(title_array)):\n",
    "    print(str(i + 1) + \" : \" +title_array[i])\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 : 남산의 부장들\n",
      "2 : 다만 악에서 구하소서\n",
      "3 : 반도\n",
      "4 : 히트맨\n",
      "5 : 테넷\n",
      "6 : 백두산\n",
      "7 : #살아있다\n",
      "8 : 강철비2: 정상회담\n",
      "9 : 담보\n",
      "10 : 닥터 두리틀\n",
      "11 : 삼진그룹 영어토익반\n",
      "12 : 정직한 후보\n",
      "13 : 도굴\n",
      "14 : 클로젯\n",
      "15 : 오케이 마담\n",
      "16 : 해치지않아\n",
      "17 : 천문: 하늘에 묻는다\n",
      "18 : 결백\n",
      "19 : 1917\n",
      "20 : 작은 아씨들\n",
      "21 : 미드웨이\n",
      "22 : 시동\n",
      "23 : 지푸라기라도 잡고 싶은 짐승들\n",
      "24 : 미스터 주: 사라진 VIP\n",
      "25 : 인비저블맨\n",
      "26 : 나쁜 녀석들: 포에버\n",
      "27 : 국제수사\n",
      "28 : 침입자\n",
      "29 : 스타워즈: 라이즈 오브 스카이워커\n",
      "30 : 스파이 지니어스 \n",
      "31 : 이웃사촌\n",
      "32 : 온워드: 단 하루의 기적\n",
      "33 : 소리도 없이\n",
      "34 : 버즈 오브 프레이(할리 퀸의 황홀한 해방)\n",
      "35 : 원더 우먼 1984\n",
      "36 : 겨울왕국 2\n",
      "37 : 오! 문희\n",
      "38 : 그린랜드\n",
      "39 : 위대한 쇼맨\n",
      "40 : 런\n",
      "41 : 뮬란\n",
      "42 : 내가 죽던 날\n",
      "43 : 기생충\n",
      "44 : 신비아파트 극장판 하늘도깨비 대 요르문간드\n",
      "45 : 프리즌 이스케이프\n",
      "46 : 검객\n",
      "47 : 조제\n",
      "48 : 사라진 시간\n",
      "49 : 밤쉘: 세상을 바꾼 폭탄선언\n",
      "50 : 알라딘\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import urllib.request as req\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://news.daum.net/digital#1\"\n",
    "\n",
    "res = req.urlopen(url)\n",
    "\n",
    "soup = BeautifulSoup(res , \"html.parser\")\n",
    "\n",
    "titles = soup.select(\"strong.tit_g > a.link_txt\")\n",
    "\n",
    "href_array = []\n",
    "title_array = []\n",
    "\n",
    "for title in titles:\n",
    "    href_array.append(title.attrs[\"href\"])\n",
    "    title_array.append(title.string.strip())\n",
    "\n",
    "for i in range(len(title_array)):\n",
    "    print(str(href_array[i]) + \" : \" + str(title_array[i]))\n",
    "    \n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "source": [
    "# 다음 티켓\n",
    "\n",
    "import urllib.request as req\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://movie.daum.net/\"\n",
    "\n",
    "url1 = \"ranking/boxoffice/weekly\"\n",
    "\n",
    "res1 = req.urlopen(url + url1)\n",
    "\n",
    "soup = BeautifulSoup(res1 , \"html.parser\")\n",
    "\n",
    "titles = soup.select(\"strong.tit_item > a.link_txt\")\n",
    "\n",
    "href_array = []\n",
    "\n",
    "title_array = []\n",
    "\n",
    "countLists = []\n",
    "\n",
    "count = 0\n",
    "for title in titles:\n",
    "    count += 1\n",
    "    href = title.attrs[\"href\"]\n",
    "    href_array.append(href)\n",
    "    title_array.append(title.string.strip())\n",
    "    countLists.append(count)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data = countLists , columns = ['Index'])\n",
    "df['Titles'] = pd.Series(title_array)\n",
    "df\n",
    "\n",
    "df.to_csv(\"DaumMovieWeekly.csv\" , sep=\",\",encoding='utf-8' , index=False)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "source": [
    "# RottenTomatoesLists\n",
    "\n",
    "import urllib.request as req\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "i_array = [\"2019\" , \"2020\" , \"2021\"]\n",
    "\n",
    "for i in range(len(i_array)):\n",
    "    url = \"https://www.rottentomatoes.com/top/bestofrt/?year=\"+i_array[i]\n",
    "\n",
    "    res = req.urlopen(url)\n",
    "\n",
    "    soup = BeautifulSoup(res , \"html.parser\")\n",
    "\n",
    "    titles = soup.select(\"td > a.unstyled\")\n",
    "\n",
    "    print(i_array[i] + \"의 순위\")\n",
    "    for title in titles:\n",
    "        print(title.string.strip())\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2019의 순위\n",
      "Parasite (Gisaengchung) (2019)\n",
      "Avengers: Endgame (2019)\n",
      "Knives Out (2019)\n",
      "Us (2019)\n",
      "Toy Story 4 (2019)\n",
      "The Irishman (2019)\n",
      "Little Women (2019)\n",
      "Booksmart (2019)\n",
      "The Farewell (2019)\n",
      "A Beautiful Day in the Neighborhood (2019)\n",
      "Marriage Story (2019)\n",
      "If Beale Street Could Talk (2019)\n",
      "Once Upon a Time In Hollywood (2019)\n",
      "Spider-Man: Far From Home (2019)\n",
      "Pain and Glory (Dolor y gloria) (2019)\n",
      "Shazam! (2019)\n",
      "Ford v Ferrari (2019)\n",
      "Dolemite Is My Name (2019)\n",
      "Apollo 11 (2019)\n",
      "The Lighthouse (2019)\n",
      "Rocketman (2019)\n",
      "Uncut Gems (2019)\n",
      "Amazing Grace (2019)\n",
      "They Shall Not Grow Old (2019)\n",
      "Ash Is Purest White (2019)\n",
      "Honeyland (2019)\n",
      "John Wick: Chapter 3 - Parabellum (2019)\n",
      "Honey Boy (2019)\n",
      "One Cut of the Dead (Kamera o tomeru na!) (2019)\n",
      "Fighting with My Family (2019)\n",
      "The Peanut Butter Falcon (2019)\n",
      "Knock Down the House (2019)\n",
      "Captain Marvel (2019)\n",
      "Birds of Passage (Pájaros de verano) (2019)\n",
      "Maiden (2019)\n",
      "Hustlers (2019)\n",
      "One Child Nation (2019)\n",
      "For Sama (2019)\n",
      "3 Faces (2019)\n",
      "Chained for Life (2019)\n",
      "First Love (Hatsukoi) (2019)\n",
      "Woman at War (Kona fer í stríð) (2019)\n",
      "How to Train Your Dragon: The Hidden World (2019)\n",
      "White Riot (2019)\n",
      "Transit (2019)\n",
      "The Chambermaid (La camarista) (2019)\n",
      "Tigers Are Not Afraid (Vuelven) (2019)\n",
      "Midnight Traveler (2019)\n",
      "Stan & Ollie (2019)\n",
      "Ready or Not (2019)\n",
      "Atlantics (2019)\n",
      "Mickey and the Bear (2019)\n",
      "Deadwood: The Movie (2019)\n",
      "Varda by Agnès (2019)\n",
      "Scream, Queen! My Nightmare on Elm Street (2019)\n",
      "Hail Satan? (2019)\n",
      "Changing the Game (2019)\n",
      "The Heiresses (Las Herederas) (2019)\n",
      "Toni Morrison: The Pieces I Am (2019)\n",
      "The Last Black Man in San Francisco (2019)\n",
      "Midsommar (2019)\n",
      "The Mustang (2019)\n",
      "Wild Rose (2019)\n",
      "Blinded by the Light (2019)\n",
      "Bad Education (2019)\n",
      "Ad Astra (2019)\n",
      "63 Up (2019)\n",
      "Homecoming: A Film by Beyoncé (2019)\n",
      "Too Late to Die Young (Tarde para morir joven) (2019)\n",
      "Gloria Bell (2019)\n",
      "By the Grace of God (Grâce à Dieu) (2019)\n",
      "Midnight Family (2019)\n",
      "Buñuel in the Labyrinth of the Turtles (Buñuel en el laberinto de las tortugas) (2019)\n",
      "Fiddler: A Miracle of Miracles (2019)\n",
      "American Factory (2019)\n",
      "Dirty God (2019)\n",
      "Making Waves: The Art of Cinematic Sound (2019)\n",
      "I'm No Longer Here (Ya no estoy aquí) (2019)\n",
      "Working Woman (2019)\n",
      "The Cave (2019)\n",
      "The Kingmaker (2019)\n",
      "I Lost My Body (2019)\n",
      "Harpoon (2019)\n",
      "Love, Antosha (2019)\n",
      "Little Woods (2019)\n",
      "Jojo Rabbit (2019)\n",
      "Dark Waters (2019)\n",
      "The Amusement Park (2019)\n",
      "The Two Popes (2019)\n",
      "Mike Wallace Is Here (2019)\n",
      "Ray & Liz (2019)\n",
      "Be Natural: The Untold Story of Alice Guy-Blaché (2019)\n",
      "Monos (2019)\n",
      "Shadow (2019)\n",
      "Rojo (2019)\n",
      "In Fabric (2019)\n",
      "An Elephant Sitting Still (Da xiang xi di er zuo) (2019)\n",
      "It Must Be Heaven (2019)\n",
      "The Mortuary Collection (2019)\n",
      "Clemency (2019)\n",
      "2020의 순위\n",
      "One Night in Miami (2020)\n",
      "Portrait of a Lady on Fire (Portrait de la jeune fille en feu) (2020)\n",
      "Minari (2020)\n",
      "Ma Rainey's Black Bottom (2020)\n",
      "Soul (2020)\n",
      "1917 (2020)\n",
      "The Invisible Man (2020)\n",
      "Never Rarely Sometimes Always (2020)\n",
      "Sound of Metal (2020)\n",
      "Promising Young Woman (2020)\n",
      "Wolfwalkers (2020)\n",
      "Hamilton (2020)\n",
      "Palm Springs (2020)\n",
      "Da 5 Bloods (2020)\n",
      "First Cow (2020)\n",
      "His House (2020)\n",
      "Miss Juneteenth (2020)\n",
      "Collective (Colectiv) (2020)\n",
      "76 Days (2020)\n",
      "Crip Camp: A Disability Revolution (2020)\n",
      "David Byrne's American Utopia (2020)\n",
      "The Trial of the Chicago 7 (2020)\n",
      "Dick Johnson Is Dead (2020)\n",
      "Host (2020)\n",
      "Possessor: Uncut (2020)\n",
      "The Forty-Year-Old Version (2020)\n",
      "Totally Under Control (2020)\n",
      "Saint Frances (2020)\n",
      "Time (2020)\n",
      "Onward (2020)\n",
      "Blow the Man Down (2020)\n",
      "The Personal History of David Copperfield (2020)\n",
      "The Vast of Night (2020)\n",
      "Welcome to Chechnya (2020)\n",
      "All In: The Fight for Democracy (2020)\n",
      "Mucho Mucho Amor: The Legend of Walter Mercado (2020)\n",
      "Relic (2020)\n",
      "The Assistant (2020)\n",
      "Athlete A (2020)\n",
      "Corpus Christi (Boze cialo) (2020)\n",
      "Another Round (Druk) (2020)\n",
      "Driveways (2020)\n",
      "On the Record (2020)\n",
      "Extra Ordinary (2020)\n",
      "The Truffle Hunters (2020)\n",
      "Slay the Dragon (2020)\n",
      "Night of the Kings (La Nuit des Rois) (2020)\n",
      "Flee (2020)\n",
      "The Dissident (2020)\n",
      "House of Hummingbird (Beolsae) (2020)\n",
      "The Fight (2020)\n",
      "A Secret Love (2020)\n",
      "Rewind (2020)\n",
      "Coded Bias (2020)\n",
      "Coup 53 (2020)\n",
      "Mayor (2020)\n",
      "Mystify: Michael Hutchence (2020)\n",
      "The Half of It (2020)\n",
      "Saint Maud (2020)\n",
      "The Painter and the Thief (2020)\n",
      "City Hall (2020)\n",
      "Anything for Jackson (2020)\n",
      "Beats (2020)\n",
      "Vitalina Varela (2020)\n",
      "Circus of Books (2020)\n",
      "Boys State (2020)\n",
      "Bacurau (Nighthawk) (2020)\n",
      "Disclosure (2020)\n",
      "Why Don't You Just Die! (Papa, sdokhni) (2020)\n",
      "Enola Holmes (2020)\n",
      "Apples (Mila) (2020)\n",
      "The Swerve (2020)\n",
      "Farewell Amor (2020)\n",
      "La Llorona (2020)\n",
      "News of the World (2020)\n",
      "Fourteen (2020)\n",
      "Slaxx (2020)\n",
      "The Go-Go's (2020)\n",
      "Make Up (2020)\n",
      "Fireball: Visitors from Darker Worlds (2020)\n",
      "The Last Tree (2020)\n",
      "Rocks (2020)\n",
      "Sweat (2020)\n",
      "Gunda (2020)\n",
      "Assassins (2020)\n",
      "There Is No Evil (Sheytan vojud nadarad) (2020)\n",
      "Babyteeth (2020)\n",
      "Shithouse (2020)\n",
      "Spontaneous (2020)\n",
      "The Human Voice (La Voz Humana) (2020)\n",
      "A Shaun the Sheep Movie: Farmageddon (2020)\n",
      "Kajillionaire (2020)\n",
      "Zappa (2020)\n",
      "Borat: Subsequent Moviefilm (2020)\n",
      "Birds of Prey (And the Fantabulous Emancipation of One Harley Quinn) (2020)\n",
      "The White Tiger (2020)\n",
      "Yes, God, Yes (2020)\n",
      "Feels Good Man (2020)\n",
      "Emma. (2020)\n",
      "Sylvie's Love (2020)\n",
      "2021의 순위\n",
      "Nomadland (2021)\n",
      "Judas and the Black Messiah (2021)\n",
      "The Father (2021)\n",
      "In the Heights (2021)\n",
      "Raya and the Last Dragon (2021)\n",
      "Summer of Soul (...Or, When the Revolution Could Not Be Televised) (2021)\n",
      "The Mitchells vs. The Machines (2021)\n",
      "A Quiet Place Part II (2021)\n",
      "MLK/FBI (2021)\n",
      "Shiva Baby (2021)\n",
      "Quo Vadis, Aida? (2021)\n",
      "Identifying Features (Sin Señas Particulares) (2021)\n",
      "Slalom (2021)\n",
      "Luca (2021)\n",
      "The Paper Tigers (2021)\n",
      "Two of Us (Deux) (2021)\n",
      "The Sparks Brothers (2021)\n",
      "Billie Eilish: The World's a Little Blurry (2021)\n",
      "Rita Moreno: Just a Girl Who Decided to Go For It (2021)\n",
      "My Heart Can't Beat Unless You Tell It To (2021)\n",
      "Drunk Bus (2021)\n",
      "Hope (Håp) (2021)\n",
      "Demon Slayer -Kimetsu no Yaiba- The Movie: Mugen Train (2021)\n",
      "The Reason I Jump (2021)\n",
      "CODA (2021)\n",
      "Riders of Justice (2021)\n",
      "Street Gang: How We Got to Sesame Street (2021)\n",
      "Plan B (2021)\n",
      "Herself (2021)\n",
      "I Carry You with Me (2021)\n",
      "Stray (2021)\n",
      "Lapsis (2021)\n",
      "Dear Comrades! (2021)\n",
      "Together Together (2021)\n",
      "The Killing of Two Lovers (2021)\n",
      "Supernova (2021)\n",
      "Test Pattern (2021)\n",
      "In the Same Breath (2021)\n",
      "Limbo (2021)\n",
      "Some Kind of Heaven (2021)\n",
      "Black Widow (2021)\n",
      "Tina (2021)\n",
      "Mass (2021)\n",
      "Nobody (2021)\n",
      "Truman & Tennessee: An Intimate Conversation (2021)\n",
      "Spring Blossom (Seize printemps) (2021)\n",
      "The Vigil (2021)\n",
      "Holler (2021)\n",
      "Final Account (2021)\n",
      "PG: Psycho Goreman (2021)\n",
      "Little Fish (2021)\n",
      "Zola (2021)\n",
      "Days of the Bagnold Summer (2021)\n",
      "All Light, Everywhere (2021)\n",
      "The Dry (2021)\n",
      "The Dig (2021)\n",
      "Cowboys (2021)\n",
      "Censor (2021)\n",
      "Rams (2021)\n",
      "Dream Horse (2021)\n",
      "Fear Street Part Two: 1978 (2021)\n",
      "Violation (2021)\n",
      "Godzilla vs. Kong (2021)\n",
      "Oxygen (2021)\n",
      "Les nôtres (2021)\n",
      "Preparations to be Together for an Unknown Period of Time (2021)\n",
      "Werewolves Within (2021)\n",
      "Operation Varsity Blues: The College Admissions Scandal (2021)\n",
      "Cruella (2021)\n",
      "The Djinn (2021)\n",
      "Come True (2021)\n",
      "Passing (2021)\n",
      "On the Count of Three (2021)\n",
      "Jakob's Wife (2021)\n",
      "I Care a Lot (2021)\n",
      "Barb & Star Go to Vista Del Mar (2021)\n",
      "In The Earth (2021)\n",
      "Bloodthirsty (2021)\n",
      "Akilla's Escape (2021)\n",
      "The Power (2021)\n",
      "Concrete Cowboy (2021)\n",
      "Notturno (2021)\n",
      "The Mauritanian (2021)\n",
      "Cliff Walkers (2021)\n",
      "Zack Snyder's Justice League (2021)\n",
      "Fear Street Part One: 1994 (2021)\n",
      "Summer of 85 (Été 85) (2021)\n",
      "The Night (2021)\n",
      "Shadow in the Cloud (2021)\n",
      "Gaia (2021)\n",
      "Knocking (2021)\n",
      "Stowaway (2021)\n",
      "To All The Boys: Always And Forever (2021)\n",
      "The Water Man (2021)\n",
      "The Map of Tiny Perfect Things (2021)\n",
      "Army Of The Dead (2021)\n",
      "Prisoners of the Ghostland (2021)\n",
      "Eight for Silver (2021)\n",
      "Jumbo (2021)\n",
      "Pixie (2021)\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}