{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 데이터 다운로드 하기\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import urllib.request\n",
    "\n",
    "#  URL 과 저장 경로 지정\n",
    "url = \"http://uta.pw/shodou/img/28/214.png\"\n",
    "savename = \"./test.png\"\n",
    "\n",
    "# 다운로드 하기\n",
    "urllib.request.urlretrieve(url,savename)\n",
    "print(\"저장 되었습니다!\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "저장 되었습니다!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import urllib.request\n",
    "\n",
    "#  URL 과 저장 경로 지정\n",
    "url = \"http://uta.pw/shodou/img/28/214.png\"\n",
    "savename = \"./test.png\"\n",
    "\n",
    "# 다운로드 하기\n",
    "\n",
    "mem = urllib.request.urlopen(url).read()\n",
    "\n",
    "#  파일로 저장하기 \n",
    "with open(savename, mode=\"wb\") as f:\n",
    "    f.write(mem)\n",
    "    print(\"저장 완료! \")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# BeautifulSoup -> Scrapping 하기\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "#  기본 사용법\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# HTML Sample\n",
    "\n",
    "html = \"\"\"\n",
    "    <html>\n",
    "        <body>\n",
    "            <h1>Header</h1>\n",
    "            <p> Line 1을 서술 </p>\n",
    "        </body>\n",
    "    </html>\n",
    "\"\"\"\n",
    "\n",
    "# HTML 분석\n",
    "soup = BeautifulSoup(html , 'html.parser')\n",
    "# print(soup)\n",
    "\n",
    "# 원하는 부분 추출하기\n",
    "h1 = soup.html.body.h1\n",
    "p1 = soup.html.body.p\n",
    "print(h1)\n",
    "\n",
    "# Text 만 출력\n",
    "print(\"h1=\" , h1.string)\n",
    "print(\"h1=\" , h1.text)\n",
    "print(\"p1=\" , p1.string)\n",
    "print(\"p1=\" , p1.text)\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<h1>Header</h1>\n",
      "h1= Header\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# id로 요소를 추출하기\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# HTML sample\n",
    "\n",
    "html = \"\"\"\n",
    "    <html>\n",
    "        <body>\n",
    "            <h1 id=\"title\">Header </h1>\n",
    "            <p id=\"body\">Line 1 description </p>\n",
    "            <span class=\"whatever\">what the fucl?</span>\n",
    "        </body>\n",
    "    </html>\n",
    "\"\"\"\n",
    "\n",
    "# HTML analystic\n",
    "soup = BeautifulSoup(html , \"html.parser\")\n",
    "\n",
    "# extract where i want\n",
    "\n",
    "title = soup.find(id= \"title\")\n",
    "body = soup.find(id=\"body\")\n",
    "\n",
    "print(title)\n",
    "print(body)\n",
    "\n",
    "# text 만 출력\n",
    "print(\"span = \", soup.find(\"span\" , {\"class\" : \"whatever\"}).text)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<h1 id=\"title\">Header </h1>\n",
      "<p id=\"body\">Line 1 description </p>\n",
      "span =  what the fucl?\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# 여러개의 요소 추출하기\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html =  \"\"\"\n",
    "    <html>\n",
    "        <body>\n",
    "            <ul>\n",
    "                <li><a href=\"http://www.naver.com\">naver</a></li>\n",
    "                <li><a href=\"http://www.daum.net\">daum</a></li>\n",
    "            </ul>\n",
    "        </body>\n",
    "    </html>\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# HTML analystic\n",
    "\n",
    "soup = BeautifulSoup(html , 'html.parser')\n",
    "links = soup.find_all('a')\n",
    "\n",
    "# 링크 목록 출력 하기\n",
    "for link in links:\n",
    "    #  해당하는 속성 들고 오기\n",
    "    hrefTag = link.attrs['href']\n",
    "    print(hrefTag)\n",
    "    \n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "http://www.naver.com\n",
      "http://www.daum.net\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "http  -> http://www.weather.go.kr/weather/forecast/mid-term-rss3.jsp?stnId=109"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 기상청 자료 활용하기\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "url = \"http://www.weather.go.kr/weather/forecast/mid-term-rss3.jsp?stnId=109\"\n",
    "\n",
    "# DATA 가져오기\n",
    "\n",
    "res = req.urlopen(url)\n",
    "soup = BeautifulSoup(res , \"html.parser\")\n",
    "\n",
    "# 원하는 데이터 추출 하기\n",
    "title = soup.find(\"title\").string\n",
    "info = soup.find(\"wf\").string\n",
    "\n",
    "infoSorts = info.split(\"<br />\")\n",
    "\n",
    "for infoSort in infoSorts:\n",
    "    print(infoSort)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CSS 선택자 사용하기\n",
    "\n",
    "soup.select_one() : css 선택자로 요소 하나를 추출\n",
    "soup.select() : css 선택자로 요소 여러개를 리스트로 추출\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = \"\"\"\n",
    "    <html>\n",
    "        <body>\n",
    "            <div id= \"meigen\">\n",
    "                <h1>위키 북스</h1>\n",
    "                <ul class=\"items\">\n",
    "                    <li>유니티 게임 이펙트 입문서</li>\n",
    "                    <li>스위프트로 시작하는 아이폰 앱 개발 교과서</li>\n",
    "                    <li>모던 웹사이트 디자인의 정석</li>\n",
    "                </ul>\n",
    "            </div>\n",
    "        </body>\n",
    "    </html>\n",
    "\"\"\"\n",
    "\n",
    "soup = BeautifulSoup(html , \"html.parser\")\n",
    "\n",
    "# 필요한 부분 css 로 추출하기\n",
    "# title 부분 추출하기\n",
    "\n",
    "\n",
    "# \"id : #\", \"class :.\"> : 자손,\" \": 후손\"\n",
    "h1 = soup.select_one(\"div#meigen > h1\")\n",
    "print(h1)\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<h1>위키 북스</h1>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "# 목록 부분 추출하기\n",
    "li_lists = soup.select(\"div#meigen > ul.items > li\")\n",
    "# print(li_lists)\n",
    "\n",
    "for li_list in li_lists:\n",
    "    print(li_list.string)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "유니티 게임 이펙트 입문서\n",
      "스위프트로 시작하는 아이폰 앱 개발 교과서\n",
      "모던 웹사이트 디자인의 정석\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "source": [
    "# 네이버 금융에서 환율 정보 추출 하기\n",
    "\n",
    "import urllib.request as req\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "url = \"https://finance.naver.com/marketindex/\"\n",
    "\n",
    "\n",
    "\n",
    "res = req.urlopen(url)\n",
    "\n",
    "soup = BeautifulSoup(res , \"html.parser\" )\n",
    "\n",
    "titles = soup.select(\"h3.h_lst > span.blind\")\n",
    "\n",
    "for title in titles[0:4]:\n",
    "    print(title.string)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "¹Ì±¹ USD\n",
      "ÀÏº» JPY(100¿£)\n",
      "À¯·´¿¬ÇÕ EUR\n",
      "Áß±¹ CNY\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "prices = soup.select(\"div.head_info > span.value\")\n",
    "price_array = []\n",
    "country_array = [\"미국 : \" , \"일본 : \" , \"유럽연합 : \" , \"중국 : \"]\n",
    "for price in prices[:4]:\n",
    "    price_array.append(price.string)\n",
    "    \n",
    "for i in range(len(price_array)):\n",
    "    print(country_array[i] , price_array[i])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "미국 :  1,143.30\n",
      "일본 :  1,035.64\n",
      "유럽연합 :  1,357.61\n",
      "중국 :  176.72\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "source": [
    "\n",
    "import urllib.request as req\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://movie.daum.net/ranking/boxoffice/yearly\"\n",
    "\n",
    "res = req.urlopen(url)\n",
    "\n",
    "soup = BeautifulSoup(res , \"html.parser\")\n",
    "\n",
    "titles = soup.select(\"a.link_txt\")\n",
    "\n",
    "title_array = []\n",
    "\n",
    "for title in titles[:50]:\n",
    "    title_array.append(title.string)\n",
    "\n",
    "for i in range(len(title_array)):\n",
    "    print(str(i + 1) + \" : \" +title_array[i])\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1 : 남산의 부장들\n",
      "2 : 다만 악에서 구하소서\n",
      "3 : 반도\n",
      "4 : 히트맨\n",
      "5 : 테넷\n",
      "6 : 백두산\n",
      "7 : #살아있다\n",
      "8 : 강철비2: 정상회담\n",
      "9 : 담보\n",
      "10 : 닥터 두리틀\n",
      "11 : 삼진그룹 영어토익반\n",
      "12 : 정직한 후보\n",
      "13 : 도굴\n",
      "14 : 클로젯\n",
      "15 : 오케이 마담\n",
      "16 : 해치지않아\n",
      "17 : 천문: 하늘에 묻는다\n",
      "18 : 결백\n",
      "19 : 1917\n",
      "20 : 작은 아씨들\n",
      "21 : 미드웨이\n",
      "22 : 시동\n",
      "23 : 지푸라기라도 잡고 싶은 짐승들\n",
      "24 : 미스터 주: 사라진 VIP\n",
      "25 : 인비저블맨\n",
      "26 : 나쁜 녀석들: 포에버\n",
      "27 : 국제수사\n",
      "28 : 침입자\n",
      "29 : 스타워즈: 라이즈 오브 스카이워커\n",
      "30 : 스파이 지니어스 \n",
      "31 : 이웃사촌\n",
      "32 : 온워드: 단 하루의 기적\n",
      "33 : 소리도 없이\n",
      "34 : 버즈 오브 프레이(할리 퀸의 황홀한 해방)\n",
      "35 : 원더 우먼 1984\n",
      "36 : 겨울왕국 2\n",
      "37 : 오! 문희\n",
      "38 : 그린랜드\n",
      "39 : 위대한 쇼맨\n",
      "40 : 런\n",
      "41 : 뮬란\n",
      "42 : 내가 죽던 날\n",
      "43 : 기생충\n",
      "44 : 신비아파트 극장판 하늘도깨비 대 요르문간드\n",
      "45 : 프리즌 이스케이프\n",
      "46 : 검객\n",
      "47 : 조제\n",
      "48 : 사라진 시간\n",
      "49 : 밤쉘: 세상을 바꾼 폭탄선언\n",
      "50 : 알라딘\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import urllib.request as req\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://news.daum.net/digital#1\"\n",
    "\n",
    "res = req.urlopen(url)\n",
    "\n",
    "soup = BeautifulSoup(res , \"html.parser\")\n",
    "\n",
    "titles = soup.select(\"strong.tit_g > a.link_txt\")\n",
    "\n",
    "href_array = []\n",
    "title_array = []\n",
    "\n",
    "for title in titles:\n",
    "    href_array.append(title.attrs[\"href\"])\n",
    "    title_array.append(title.string.strip())\n",
    "\n",
    "for i in range(len(title_array)):\n",
    "    print(str(href_array[i]) + \" : \" + str(title_array[i]))\n",
    "    \n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "source": [
    "# 다음 티켓\n",
    "\n",
    "import urllib.request as req\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://movie.daum.net/\"\n",
    "\n",
    "url1 = \"ranking/boxoffice/weekly\"\n",
    "\n",
    "res1 = req.urlopen(url + url1)\n",
    "\n",
    "soup = BeautifulSoup(res1 , \"html.parser\")\n",
    "\n",
    "titles = soup.select(\"strong.tit_item > a.link_txt\")\n",
    "\n",
    "href_array = []\n",
    "\n",
    "title_array = []\n",
    "\n",
    "countLists = []\n",
    "\n",
    "count = 0\n",
    "for title in titles:\n",
    "    count += 1\n",
    "    href = title.attrs[\"href\"]\n",
    "    href_array.append(href)\n",
    "    title_array.append(title.string.strip())\n",
    "    countLists.append(count)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data = countLists , columns = ['Index'])\n",
    "df['Titles'] = pd.Series(title_array)\n",
    "df\n",
    "\n",
    "df.to_csv(\"DaumMovieWeekly.csv\" , sep=\",\",encoding='utf-8' , index=False)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "source": [
    "# RottenTomatoesLists\n",
    "\n",
    "import urllib.request as req\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "i_array = [\"2019\" , \"2020\" , \"2021\"]\n",
    "\n",
    "for i in range(len(i_array)):\n",
    "    url = \"https://www.rottentomatoes.com/top/bestofrt/?year=\"+i_array[i]\n",
    "\n",
    "    res = req.urlopen(url)\n",
    "\n",
    "    soup = BeautifulSoup(res , \"html.parser\")\n",
    "\n",
    "    titles = soup.select(\"td > a.unstyled\")\n",
    "    ratings = soup.select(\"span.tMeterScore\")\n",
    "    \n",
    "    print(i_array[i] + \"의 순위\")\n",
    "\n",
    "    title_array = []\n",
    "    rating_array = []\n",
    "    count = 0\n",
    "    count_array = []\n",
    "    for title in titles:\n",
    "        count += 1\n",
    "        title_array.append(title.string.strip())\n",
    "        count_array.append(count)\n",
    "\n",
    "    for rating in ratings:\n",
    "        rating_array.append(rating.string)\n",
    "\n",
    "    df = pd.DataFrame(data = count_array , columns = ['Ranking'])\n",
    "    df['Rating'] = pd.Series(rating_array)\n",
    "    df['Titles'] = pd.Series(title_array)\n",
    "    \n",
    "    df.to_csv(\"RTMovieWeekly\"+i_array[i] +\".csv\" , sep=\",\",encoding='utf-8' , index=False)\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2019의 순위\n",
      "2020의 순위\n",
      "2021의 순위\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# MYSQL 연결"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "source": [
    "import pymysql as sql\n",
    "\n",
    "conn = sql.connect(host=\"127.0.0.1\",user=\"root\" , passwd = \"qwer1234\" , db=\"education\" , charset=\"utf8\")\n",
    "\n",
    "curs = conn.cursor()\n",
    "\n",
    "sqlSyntax = \"select * from student\"\n",
    "\n",
    "curs.execute(sqlSyntax)\n",
    "\n",
    "# DATA Fetch\n",
    "rows = curs.fetchall()\n",
    "# print(rows)\n",
    "\n",
    "\n",
    "# list로 변환\n",
    "# 한번에 리턴 해주기 때문에 tuple 이라고 인지 하고 있다.\n",
    "rowList = list(rows)\n",
    "print(rowList)\n",
    "\n",
    "conn.close()\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('123', '오낑깡', '심리', '010203040', None), ('S001', '박소명', '컴퓨터공학과', '123-4567', None), ('S002', '최민국', '컴퓨터공학과', '234-5678', None), ('S005', '김상진', '사학과', '567-8901', None), ('S006', '황정숙', '사학과', '678-9012', None), ('z999', 'wjsn', 'skjxz', '02929384', None)]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#  select의 내용을 dataframe으로 보기"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = create_engine(\"mysql+pymysql://root:qwer1234@127.0.0.1:3306/education?\")\n",
    "conn = engine.connect()\n",
    "\n",
    "data = pd.read_sql_table('student' , conn)\n",
    "data\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  scode sname   sdept     sphone saddress\n",
       "0   123   오낑깡      심리  010203040     None\n",
       "1  S001   박소명  컴퓨터공학과   123-4567     None\n",
       "2  S002   최민국  컴퓨터공학과   234-5678     None\n",
       "3  S005   김상진     사학과   567-8901     None\n",
       "4  S006   황정숙     사학과   678-9012     None\n",
       "5  z999  wjsn   skjxz   02929384     None"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scode</th>\n",
       "      <th>sname</th>\n",
       "      <th>sdept</th>\n",
       "      <th>sphone</th>\n",
       "      <th>saddress</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123</td>\n",
       "      <td>오낑깡</td>\n",
       "      <td>심리</td>\n",
       "      <td>010203040</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S001</td>\n",
       "      <td>박소명</td>\n",
       "      <td>컴퓨터공학과</td>\n",
       "      <td>123-4567</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S002</td>\n",
       "      <td>최민국</td>\n",
       "      <td>컴퓨터공학과</td>\n",
       "      <td>234-5678</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S005</td>\n",
       "      <td>김상진</td>\n",
       "      <td>사학과</td>\n",
       "      <td>567-8901</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S006</td>\n",
       "      <td>황정숙</td>\n",
       "      <td>사학과</td>\n",
       "      <td>678-9012</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>z999</td>\n",
       "      <td>wjsn</td>\n",
       "      <td>skjxz</td>\n",
       "      <td>02929384</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 147
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# dataFrame을 Database 로 insert 하기"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "source": [
    "column = ['rank' , 'title']\n",
    "\n",
    "movies = pd.read_csv(\"DaumMovieWeekly.csv\")\n",
    "movies.columns = column\n",
    "movies.head()\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   rank        title\n",
       "0     1       블랙 위도우\n",
       "1     2         발신제한\n",
       "2     3         크루엘라\n",
       "3     4  콰이어트 플레이스 2\n",
       "4     5           랑종"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>블랙 위도우</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>발신제한</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>크루엘라</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>콰이어트 플레이스 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>랑종</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 148
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "source": [
    "\n",
    "engine = create_engine(\"mysql+pymysql://root:qwer1234@127.0.0.1:3306/daum?\")\n",
    "conn = engine.connect()\n",
    "\n",
    "movies.to_sql(name='daumlist' , con=conn ,if_exists='append' , index=False)\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "source": [
    "column = ['rank' ,'rate' ,'title']\n",
    "\n",
    "rottenMovies = pd.read_csv(\"RTMovieWeekly2021.csv\")\n",
    "rottenMovies.columns = column\n",
    "rottenMovies.to_sql(name='rotten_tomatoes' , con = conn , if_exists='append' , index = False)\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}